{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2: Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will implement and evaluate a simple system for information extraction. The task of the system is to read sentences and extract entity pairs of the form *x*&ndash;*y* where *x*&nbsp;is a person, *y*&nbsp;is an organisation, and *x* is the &lsquo;leader&rsquo; of&nbsp;*y*. Consider the following example sentence:\n",
    "\n",
    "<blockquote>\n",
    "Mr. Obama also selected Lisa Jackson to head the Environmental Protection Agency.\n",
    "</blockquote>\n",
    "\n",
    "From this sentence the system should extract the pair\n",
    "```\n",
    "(\"Lisa Jackson\", \"Environmental Protection Agency\")\n",
    "```\n",
    "\n",
    "The system will have to solve the following sub-tasks:\n",
    "* entity extraction &ndash; identifying mentions of person entities in text\n",
    "* relation extraction &ndash; identifying instances of the &lsquo;is-leader-of&rsquo; relation\n",
    "\n",
    "The data set for the lab consists of 62,010&nbsp;sentences from the [Groningen Meaning Bank](http://gmb.let.rug.nl) (release 2.2.0), an open corpus of English. To analyse the sentences you will use [spaCy](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cell imports the Python module required for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is contained in the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"/home/TDDE16/labs/l2/data/gmb.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tm2` module defines a function `read_data` that returns an iterator over the lines in a file. You should use this function to read the data for this lab. Use the optional argument `n` to restrict the iteration to the first few lines of the file. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked assailants with grenades and automatic weapons attacked a wedding party in southeastern Turkey, killing 45 people and wounding at least six others.\n",
      "Turkish officials said the attack occurred Monday in the village of Bilge about 600 kilometers from Ankara.\n",
      "The wounded were taken to the hospital in the nearby city of Mardin.\n"
     ]
    }
   ],
   "source": [
    "for sentence in tm2.read_data(data_file, n=3):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell imports spaCy and loads its English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en', disable=['textcat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the entity extraction part of your system, you do not need to do much, as you can use the full natural language processing power built into spaCy. The following code extracts the entities from the first 5&nbsp;sentences of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turkey\t13\t14\tGPE\n",
      "45\t16\t17\tCARDINAL\n",
      "at least six\t20\t23\tCARDINAL\n",
      "Turkish\t0\t1\tNORP\n",
      "Monday\t6\t7\tDATE\n",
      "Bilge\t11\t12\tORG\n",
      "about 600 kilometers\t12\t15\tQUANTITY\n",
      "Ankara\t16\t17\tGPE\n",
      "Mardin\t12\t13\tORG\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(nlp.pipe(tm2.read_data(data_file, n=5))):\n",
    "    for ent in doc.ents:\n",
    "        print(\"{}\\t{}\\t{}\\t{}\".format(ent.text, ent.start, ent.end, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the [section about named entities](https://spacy.io/usage/linguistic-features#section-named-entities) from spaCy&rsquo;s documentation to get some background on this. (Please note that we are using version&nbsp;1 of the spaCy library, which means that there may be slight differences in the usage. At the time of writing, the current version&nbsp;2 is not yet stable and fast enough for this lab.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Extract relevant pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first problem that you will have to solve is to identify pairs of entities that are in the &lsquo;is-leader-of&rsquo; relation, as in the example above. There are many ways to do this, but for this lab it suffices to implement the strategy outlined in the section on [Relation Extraction](http://www.nltk.org/book/ch07.html#relation-extraction) in the book by Bird, Klein, and Loper (2009):\n",
    "\n",
    "* look for all triples of the form $(X, \\alpha, Y)$ where $X$ and $Y$ are named entities of type *person* and $\\alpha$ is the intervening text\n",
    "* write a regular expression to match just those instances of $\\alpha$ that express the &lsquo;is-leader-of&rsquo; relation\n",
    "\n",
    "You can restrict your attention to adjacent pairs of entities &ndash; that is, cases where $X$ precedes $Y$ and $\\alpha$ does not contain other named entities.\n",
    "\n",
    "Write a function `extract` that takes an analysed sentence (represented as a spaCy [`Doc`](https://spacy.io/api/doc) object) and yields pairs $(X, Y)$ of strings representing entity mentions predicted to be in the &lsquo;is-leader-of&rsquo; relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract(doc):\n",
    "    \"\"\"Extract relevant relation instances from the specified document.\n",
    "    \n",
    "    Args:\n",
    "        doc: The sentence as analysed by spaCy.\n",
    "    Yields:\n",
    "        Pairs of strings representing the extracted relation instances.\n",
    "    \"\"\"\n",
    "\n",
    "    L = ['head of', 'leader', 'leading', 'heads']\n",
    "    PAIR = []\n",
    "    \n",
    "    org = ''\n",
    "    person = ''\n",
    "    \n",
    "    for i, ent in enumerate(doc.ents):\n",
    "        if ent.label_ == \"ORG\" and person:\n",
    "            org = re.sub(r'([()])', r'\\\\\\1', ent.text)\n",
    "        \n",
    "        if ent.label_ == \"PERSON\":\n",
    "            person = re.sub(r'([()])', r'\\\\\\1', ent.text)\n",
    "        \n",
    "        if org and person:\n",
    "            PAIR.append((person, org))\n",
    "            person = ''\n",
    "            org = ''\n",
    "\n",
    "    if not PAIR:\n",
    "        return\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for person, org in PAIR:\n",
    "        try:\n",
    "            results.extend(re.findall(r\"(\"+person+r\").* (\"+'|'.join(L)+r\") .*(\"+org+r\")\", doc.text))\n",
    "        except:\n",
    "            print('ERROR', person, org)\n",
    "    \n",
    "    for person, relation, org in results:\n",
    "        if not person or not org: continue\n",
    "        \n",
    "        yield (person, org)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows how your function is supposed to be used. The code prints out the extracted pairs for the first 1,000&nbsp;sentences in the data. It additionally numbers each pair with the identifier of the sentence (line number in the data file) which it was extracted from. Note that the sentence (line) numbering starts at index&nbsp;0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS [('Aung San Suu Kyi', 'head of', 'the National League for Democracy')]\n",
      "DOC Aung San Suu Kyi, the head of the National League for Democracy, has spent 10 of the past 16 years in detention, mostly under house arrest.\n",
      "512\tAung San Suu Kyi\tthe National League for Democracy\n",
      "RESULTS [('Viktor Yanukovych', 'leader', 'Russian Party')]\n",
      "DOC Representatives of the country's pro-Western coalition say talks scheduled for Monday were canceled after Viktor Yanukovych, the leader of the opposition pro- Russian Party of Regions failed to attend.\n",
      "736\tViktor Yanukovych\tRussian Party\n",
      "RESULTS [('Asif Ali Zardari', 'leader', \"the Pakistan People's Party\")]\n",
      "DOC Asif Ali Zardari, leader of the Pakistan People's Party, says he wants to set aside the dispute over the Kashmir region so the two countries can focus on other issues, including boosting trade.\n",
      "802\tAsif Ali Zardari\tthe Pakistan People's Party\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(nlp.pipe(tm2.read_data(data_file, n=1000))):\n",
    "    for person, org in extract(doc):\n",
    "        print(\"{}\\t{}\\t{}\".format(i, person, org))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you feel confident that your `extract` function does what it is supposed to do, execute the following cell to extract the entities from the full data set. Note that this will probably take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = set()\n",
    "for i, doc in enumerate(nlp.pipe(tm2.read_data(data_file))):\n",
    "    for person, org in extract(doc):\n",
    "        extracted.add((i, person, org))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the above cell, all extracted id-string-string triples are in the set `extracted`. The code in the next cell will print the first 10&nbsp;triples in this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\tAung San Suu Kyi\tthe National League for Democracy\n",
      "736\tViktor Yanukovych\tRussian Party\n",
      "802\tAsif Ali Zardari\tthe Pakistan People's Party\n",
      "1591\tFidel Castro\tthe Communist Party\n",
      "2297\tAbdul Aziz al-Hakim\tthe Supreme Council for the Islamic Revolution in Iraq\n",
      "4567\tBush\tthe U.S. Justice Department\n",
      "8206\tJ. Patrick Boyle\tthe American Meat Institute\n",
      "9004\tJoschka Fischer\tthe Green Party\n",
      "9021\tHassan Halemi\tKabul University\n",
      "9047\tKarzai\tTaleban\n"
     ]
    }
   ],
   "source": [
    "for i, person, org in sorted(extracted)[:10]:\n",
    "    print(\"{}\\t{}\\t{}\".format(i, person, org))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Evaluate your system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have an extractor, but how good is it? To help you answer this question, we provide you with a &lsquo;gold standard&rsquo; of entity pairs that your system should be able to extract. The following code loads them (again augmented with the relevant sentence id) from the file `gold.txt` and adds them to the set `gold`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_file = \"/home/TDDE16/labs/l2/data/gold.txt\"\n",
    "\n",
    "gold = set()\n",
    "with open(gold_file) as fp:\n",
    "    for line in fp:\n",
    "        columns = line.rstrip().split('\\t')\n",
    "        gold.add((int(columns[0]), columns[1], columns[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code prints the 10&nbsp;first pairs from the gold standard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802\tAli Zardari\tPakistan People 's Party\n",
      "2297\tAbdul Aziz al-Hakim\tSupreme Council\n",
      "4823\tSlavkov\tBulgarian National Olympic Committee\n",
      "7902\tMr. Hakim\tSupreme Council\n",
      "8206\tJ. Patrick Boyle\tAmerican Meat Institute\n",
      "8633\tAli Rodriguez\tPetroleos de Venezuela\n",
      "9004\tForeign Minister Joschka Fischer\tGreen Party\n",
      "11021\tKhalaf\tal-Qaida\n",
      "11259\tJoseph Domenech\tU.N. 's Food and Agricultural Organization\n",
      "13043\tDavid Petraeus\tU.S. Central Command\n"
     ]
    }
   ],
   "source": [
    "for i, person, org in sorted(gold)[:10]:\n",
    "    print(\"{}\\t{}\\t{}\".format(i, person, org))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task now is to write code that computes the precision, recall, and F1 measure of your extractor relative to the gold standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reference, predicted):\n",
    "    \"\"\"Print out the precision, recall, and F1 for the id-entity-entity\n",
    "    triples in the set `predicted`, given the triples in the reference set.\n",
    "    \n",
    "    Args:\n",
    "        reference: The reference set of triples.\n",
    "        predicted: The set of predicted triples.\n",
    "    Returns:\n",
    "        Nothing, but prints out precision, recall, and F1.\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    \n",
    "    for pred in predicted:\n",
    "        if pred in reference:\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            false_positive += 1\n",
    "            \n",
    "    for ref in reference:\n",
    "        if ref not in predicted:\n",
    "            false_negative += 1\n",
    "\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    def format(proc):\n",
    "        return str(round(proc*100, 2)) + '%'\n",
    "            \n",
    "    print('P: ' + format(precision))\n",
    "    print('R: ' + format(recall))\n",
    "    print('F: ' + format(f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell shows how your function is intended to be used, as well as the suggested output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 5.83%\n",
      "R: 13.04%\n",
      "F: 8.05%\n"
     ]
    }
   ],
   "source": [
    "evaluate(gold, extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Entity resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results of your quantitative evaluation, you will realise that your extractor (probably) does a rather poor job in matching the gold standard. One reason for this is that the NLP preprocessing is not perfect (spaCy was not trained on the annotations in the Groningen Meaning Bank), and that the approach of using regular expressions for relation extraction is rather naive.\n",
    "\n",
    "Another reason however is that the current version of your system does not include a component for *entity resolution*. To give an example, your system does not realise that the strings `David Petraeus` and `General David Petraeus` refer to the same entity.\n",
    "\n",
    "While writing an entity resolver is beyond the scope of this assignment, we ask you to *simulate* such a resolver. More specifically, you should implement a function `normalise` that takes an entity mention (a string) as its input and rewrites it to the form used in the gold standard. While in some sense this is &lsquo;cheating&rsquo;, it allows you to assess the performance of a more realistic system.\n",
    "\n",
    "The following cell contains skeleton code for the `normalise` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(text):\n",
    "    if text == \"Asif Ali Zardari\":\n",
    "        return \"Ali Zardari\"\n",
    "    \n",
    "    if text == \"the Supreme Council for the Islamic Revolution in Iraq\":\n",
    "        return \"Supreme Council\"\n",
    "    \n",
    "    if text == \"Joschka Fischer\":\n",
    "        return \"Foreign Minister Joschka Fischer\"\n",
    "    \n",
    "    if text == \"Resistance Army\":\n",
    "        return \"Lord 's Resistance Army\"\n",
    "    \n",
    "    if text == \"U.N.\":\n",
    "        return \"U.N. 's Food and Agricultural Organization\"\n",
    "    \n",
    "    if text == \"Chen Shui-bian\":\n",
    "        return \"President Chen Shui-bian\"\n",
    "    \n",
    "    if text == 'the Yisrael Beitenu party':\n",
    "        return 'Yisrael Beitenu'\n",
    "    \n",
    "    if text == 'Mwanawasa':\n",
    "        return 'Mr. Mwanawasa'\n",
    "    \n",
    "    if text == 'Zarqawi':\n",
    "        return 'al-Zarqawi'\n",
    "    \n",
    "    if text == 'Abbas':\n",
    "        return 'Mr. Abbas'\n",
    "    \n",
    "    if text == 'the Movement of Islamic Reform in Arabia':\n",
    "        return 'Movement of Islamic Reform'\n",
    "    \n",
    "    if text == 'Rafsanjani':\n",
    "        return 'Mr. Rafsanjani'\n",
    "    \n",
    "    tokens = text.split(' ')\n",
    "    if tokens[0] == \"the\":\n",
    "        return ' '.join(tokens[1:])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell shows how `normalise` is intended to be used. Each triple in the set `extracted` is transformed by feeding the two entity mentions into the `normalise` function. The normalised triples are then added to a new set `extracted_normalised`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_normalised = set()\n",
    "for triple in extracted:\n",
    "    extracted_normalised.add((triple[0], normalise(triple[1]), normalise(triple[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass the assignment, you should add enough normalisation rules to `normalise` to achieve a recall of at least 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 25.24%\n",
      "R: 56.52%\n",
      "F: 34.9%\n"
     ]
    }
   ],
   "source": [
    "evaluate(gold, extracted_normalised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Limitations of the gold standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entity pair in the gold standard has been manually checked for correctness. However, there is no guarantee that the gold standard contains all relevant pairs &ndash; there are in fact many pairs that are missing from the gold standard. Your last task in this assignment is to find at least 5&nbsp;entity pairs in the data that are valid instances of the &lsquo;is-leader-of&rsquo; relation but are not contained in the gold standard.\n",
    "\n",
    "You can solve this task either by writing code or by manual work (inspecting the data file), or mix the two strategies. In any case, you should enter your pairs in the textbox below. Use the triple format shown above where for each pair you also specify the sentence id (line number in the data file) from which the instance was extracted."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentence_id    entity 1            entity 2\n",
    "512            Aung San Suu Kyi    National League for Democracy\n",
    "1591           Fidel Castro        Communist Party\n",
    "9710           Robert Mugabe       Zanu-PF\n",
    "12291          al-Jaafari          Dawa party\n",
    "54220          Ndjabu              Integrationist Front\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we ask you to reflect on the limitations of the evaluation that you carried out in this lab and discuss the question: *How should systems for information extraction really be evaluated?*. Here are some starting points for your discussion.\n",
    "\n",
    "* How could one create a better gold standard for this task?\n",
    "* What do precision, recall, and F1 actually measure in this context?\n",
    "* What measures would be more suitable to evaluate this task?\n",
    "* What other ways of evaluating systems for information extraction can you think of?\n",
    "\n",
    "Submit your discussion as a short text (ca. 250&nbsp;words). When presenting your arguments, link back to your own results and experience from this lab, and to concepts you have learned in the lectures or in other parts of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "Would be nice if the gold standard was more correct, i.e. contained more true positives. Also the gold standard should use the same name for one entity, for example David Petreus/General Petreus/General David Petreus. Or if it accepted multiple names.\n",
    "\n",
    "It was good that the measurement required to fulfill was recall since it doesnt depend on false positives, only false negatives. It would perhaps be easy to fake it by just adding all PERSON-ORG pairs, but its in that case precision would be low instead. For this lab, using precision is an issue because the gold standard does not include all pairs.\n",
    "\n",
    "In a real life application it would be better to use some extrinsic evaluation in order to find the real value of the work put into the entity extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
